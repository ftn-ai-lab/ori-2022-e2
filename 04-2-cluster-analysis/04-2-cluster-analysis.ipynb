{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mašinsko učenje\n",
    "\n",
    "Umesto priče o tome šta je mašinsko učenje, jedna slika govori hiljadu reči ;)\n",
    "\n",
    "![img/ml_one_slide.jpg](img/ml_one_slide.jpg)\n",
    "\n",
    "Dva najčešće korišćena tipa mašinskog učenja su:\n",
    "* Nadgledano učenje (*eng. supervised learning*)\n",
    "* Nenadgledano učenje (*eng. unsupervised learning*)\n",
    "\n",
    "---\n",
    "\n",
    "## Nadgledano učenje\n",
    "\n",
    "Učenje funkcije na osnovu obeleženih podataka, odnosno predviđanje *y* u odnosu na *x* iz primera *(x,y)* (setite se linearne regresije). Performanse postupka se mere određenom metrikom/greškom. Veliki broj postupaka, neki od najpopularnijih su:\n",
    "* Veštačke neuronske mreže\n",
    "* Stabla odlučivanja + Random forests\n",
    "* Naivni Bayes klasifikator\n",
    "* kNN (k nearest neighbors)\n",
    "* SVM (support vector machine)\n",
    "* ... i još mnogo drugih ...\n",
    "\n",
    "*Više o nadgledanom učenju sledeći čas!*\n",
    "\n",
    "---\n",
    "\n",
    "## Nenadgledano učenje\n",
    "\n",
    "Učenje funkcije koja opisuje \"skrivenu\" strukturu neobeleženih podataka, odnosno učenje reprezentacije podataka na osnovu primera *x* (nema *y*). Nenadgledano učenje može biti korisno za pronalaženje interesantnih veza među podacima. Ne postoji neki standardni način merenja performansi. Postupci se dele na:\n",
    "* Klasterizacija/klasterovanje (*eng. clustering*) - grupisanje podataka na osnovu sličnosti\n",
    "* Analiza komponenti (*eng. component analysis*) - otkrivanje najdeskriptivnikih osobina podataka\n",
    "* Asocijativna pravila (*eng. association rules*) - pronalaženje uobičajenih kombinacija osobina podataka\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### K-means\n",
    "\n",
    "Jedan od najčešće korišćenih algoritama za nenadgledano klasterovanje podataka. Preciznije, k-means je ne-hijerarhijska metoda grupisanja sličnih podataka. K-means je tehnika koja se često koristi u tzv. *eksplorativnoj analizi podataka*.\n",
    "\n",
    "Klaterizacija je zadatak grupisanja skupa objekata, tako da su objekti koji su u istoj grupi (odnosno *klasteru*) sličniji (u nekom smislu) jedni drugima, više nego što su slični objektima u drugim grupama (klasterima).\n",
    "\n",
    "Pseudo-kod:\n",
    "\n",
    "```\n",
    "za svaku grupu inicijalizovati nasumično centar\n",
    "dok se centri ne prestanu kretati ili ne dostigne max broj iteracija:\n",
    "    pridruži svaki element grupi sa njemu najbližim centrom grupe\n",
    "    pomeri centar svih grupa na osnovu novih elemenata\n",
    "```\n",
    "\n",
    "![img/kmeans.gif](img/kmeans.gif)\n",
    "\n",
    "** TODO 1: ** Implementirati K-means algoritam (u ```src/kmeans.py``` sve što je označeno sa TODO 1):\n",
    "  * Metodu ```fit``` klase ```KMeans```, koja kao parametar prima n-dimenzionalne podatke koje treba klasterizovati.\n",
    "  * Metodu ```predict``` klase ```KMeans```, koja kao parametar prima jedan podatak i vraća indeks klastera kojem pripada (na osnovu euklidske udaljenosti).\n",
    "  * Metodu ```recalculate_center``` klase ```Cluster```, koja treba da izračuna centar klastera kao srednju vrednost svih podataka u njemu i da vrati ovu vrednost kao rezultat.\n",
    "  * ** Za domaći ** - proširiti metodu ```fit``` tako da se algoritam zaustavi kada se centri klastera više ne pomeraju.\n",
    "  \n",
    "** ТОDO 2: ** Primeniti K-means algoritam na Iris data setu (https://en.wikipedia.org/wiki/Iris_flower_data_set), u cilju pronalaženja dva klastera u odnosu na širinu sepala i dužinu petala. Pokrenuti skriptu ```src/iris.py```.\n",
    "\n",
    "\n",
    "![img/iris.png](img/iris.png)\n",
    "\n",
    "\n",
    "#### Određivanje optimalnog K\n",
    "\n",
    "Kako znati unapred koliko ima klastera? Lako je videti kada su podaci dvodimenzionalni, jer ih je onda lako i vizualizovati, ali često podaci imaju (mnogo) više od samo 2 dimenzije - ovo je tzv. \"kletva dimenzionalnosti\" (*eng. curse of dimensionality*) u mašinskom učenju.\n",
    "\n",
    "Određivanje optimalnog K (tj. broja klastera) je nešto se dosta proučavalo, a mi ćemo koristi tzv. \"metodu lakta\" (*eng. elbow method*). Za određen broj K (npr. 2, 4, 6, 8, ..., 20) se vrši klasterizacija i zatim se računa suma kvadratnih grešaka (SSE). SSE se računa tako što se unutar svakog klastera sumiraju kvadrati udaljenosti podataka od centra klastera, i zatim se sve to opet sumira. Matematički:\n",
    "\n",
    "$ SSE = \\sum_{i=1}^{K} \\sum_{x \\in c_{i}} dist(x, c_{i})^{2} $, gde je *dist* euklidska udaljnost.\n",
    "\n",
    "Zatim se za sve plotuje SSE u odnosu na K, npr.:\n",
    "\n",
    "![img/sse.png](img/sse.png)\n",
    "\n",
    "** TODO 3: ** Implementirati izračunavanje sume kvadratnih grešaka (SSE) u metodi ```sum_squared_error``` klase ```KMeans```. Zatim ponovo pokrenuti ```src/iris.py``` skriptu i odrediti optimalno K na osnovu grafika.\n",
    "\n",
    "** TODO 4: ** Normalizovati podatke pre primene K-means algoritma. Normalizacija je potrebna kada su opsezi vrednosti u podacima različiti (npr. ako se visina ljudi meri u metrima, a težina u gramima). Normalizacija se može vršiti na više načina, dva najčešća su:\n",
    "* Svođenje podataka na isti interval, npr. [-1, 1]. Šta je neophodno znati o podacima za ovakvu normalizaciju?\n",
    "* Centriranje i normalizacija standardnom devijacijom -> x = (x - srednja_vrednost(x)) / standardna_devijacija(x). Ovakva normalizacije se najčešće koristi i nije karakteristična samo za K-means, već generalno za analizu podataka u mašinskom učenju.\n",
    "\n",
    "** TODO 5: ** Primeniti K-means na podatke generisane u ```src/ball_circle.py```.\n",
    "\n",
    "---\n",
    "\n",
    "#### Prednosti K-means\n",
    "\n",
    "* Jednostavan i lako razumljiv\n",
    "* Laka implementacija\n",
    "* Relativno dobre performanse (za malo K)\n",
    "* Odličan kada su klasteri sferičnog/globularnog oblika (malo formalnije hiper-sferičnog, za sfere u >3 dimenzija)\n",
    "\n",
    "#### Mane K-means\n",
    "* Potrebno unapred znati K (što je nekad teško odrediti)\n",
    "* Nije deterministički - pošto se centri inicijalizuju nasumično, nekad se dobijaju drugačiji rezultati\n",
    "* Osetljiv na šum\n",
    "* Kada podaci nisu globularnog oblika -> beskoristan (pogledati donju sliku)\n",
    "* Nema mogućnost hijerarhijskog klasterovanja (razlikovanje više manjih podklastera unutar većeg klastera)\n",
    "\n",
    "![img/kmeans_fail.png](img/kmeans_fail.png)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### DBSCAN\n",
    "\n",
    "DBSCAN (Density-based spatial clustering of applications with noise) je takođe algoritam za klasterizaciju podataka. Ovaj postupak se zasniva na ideji grupisanja tačaka (podataka) na osnovu njihove međusobne udaljenosti. Ukoliko se tačke nalaze u tzv. *epsilon okolini* one su deo nekog klastera, u suprotnom se posmatraju kao šum.\n",
    "\n",
    "Iako predstavljen još 1996. godine, 2014. godine na jednoj od najprestižnijih konferencija (KDD), DBSCAN je nagrađen nagradom \"testa vremena\", kao jedan od najviše citiranih algoritama sa ogromnom upotrebom, kako u teoriji tako i u praksi.\n",
    "\n",
    "\n",
    "Opis DBSCAN algoritma:\n",
    "1. Neka postoji neki skup tačaka (podataka) koje želimo da klasterizujemo. U samom postupku, razlikuju se tri vrste tačaka: ključne tačke, dostupne tačke i šum.\n",
    "2. DBSCAN zahteva dva parametra: *epsilon* (eps) i *minimalni broj potrebnih tačaka koje čine region* (minPts). Epsilon okolina se najčešće računa korišćenjem euklidske udaljenosti.\n",
    "3. Algoritam počinje sa proizvoljnom tačkom. Računa se epsilon okolina te tačke, i ukoliko se u njoj nalazi dovoljno tačaka (minPts), započinje se novi klaster. U suprotnom, tačka se računa kao šum. Obratiti pažnju da tačka, iako je šum, kasnije *može* biti pronađena kao deo neke druge epsilon okoline sa dovoljno tačaka i samim tim da postane deo klastera.\n",
    "4. Ukoliko je za tačku određeno da pripada klasteru, sve tačke u njenoj epsilon okolini takođe pripadaju tom klasteru. Dakle, sve tačke koje su pronađene u epsilon okolini trenutne tačke se dodaju u klaster, kao i tačke koje se nalaze u epsilon okolini tih tačaka (rekurzivno). Proces se nastavlja dok se ne nađe ceo klaster, odnosno dok se ne obiđu sve tačke u epsilon okolinama. \n",
    "5. Onda se nalazi nova, prozivoljna neposećena tačka, za koju se ponavlja čitav postupak, što dovodi do otkrivanja ili novog klastera ili šuma.\n",
    "\n",
    "![img/dbscan.png](img/dbscan.png)\n",
    "\n",
    "Pseudo-kod algoritma (preuzet sa Wikipedia stranice):\n",
    "\n",
    "```\n",
    "DBSCAN(D, eps, MinPts) {\n",
    "   C = 0\n",
    "   for each point P in dataset D {\n",
    "      if P is visited\n",
    "         continue next point\n",
    "      mark P as visited\n",
    "      NeighborPts = regionQuery(P, eps)\n",
    "      if sizeof(NeighborPts) < MinPts\n",
    "         mark P as NOISE\n",
    "      else {\n",
    "         C = next cluster\n",
    "         expandCluster(P, NeighborPts, C, eps, MinPts)\n",
    "      }\n",
    "   }\n",
    "}\n",
    "\n",
    "expandCluster(P, NeighborPts, C, eps, MinPts) {\n",
    "   add P to cluster C\n",
    "   for each point P' in NeighborPts { \n",
    "      if P' is not visited {\n",
    "         mark P' as visited\n",
    "         NeighborPts' = regionQuery(P', eps)\n",
    "         if sizeof(NeighborPts') >= MinPts\n",
    "            NeighborPts = NeighborPts joined with NeighborPts'\n",
    "      }\n",
    "      if P' is not yet member of any cluster\n",
    "         add P' to cluster C\n",
    "   }\n",
    "}\n",
    "\n",
    "regionQuery(P, eps)\n",
    "   return all points within P's eps-neighborhood (including P)\n",
    "```\n",
    "\n",
    "** TODO 6: ** Implementirati DBSCAN algoritam u metodi ```fit``` klase ```DBSCAN```, ```src/dbscan.py```.\n",
    "\n",
    "** TOOD 7: ** Primeniti DBSCAN nad Iris podacima ```src/iris.py``` i nad podacima u skripti ```src/ball_circle.py```.\n",
    "\n",
    "---\n",
    "\n",
    "#### Prednosti DBSCAN\n",
    "\n",
    "* Nije potrebno unapred znati broj klastera (kao kod K-means)\n",
    "* Klasteri mogu biti proizvoljnog oblika\n",
    "* Ume da tretira šum\n",
    "* Parametre epsilon i minPts je lako menjati u cilju dobijanja klastera različitih veličina i oblika, i ove parametre često podešavaju eksperti sa domenskim znanjem\n",
    "\n",
    "\n",
    "#### Mane DBSCAN\n",
    "\n",
    "* Kvalitet rezultata zavisi od toga čime se meri epsilon. Obično je to euklidska udaljenost, ali za višedimenzionalne podatke potrebne su drugačije metrike\n",
    "* Kada postoje varijacije u gustini klastera, nemoguće je odrediti epsilon i minPts da odgovara svim klasterima\n",
    "* U slučaju kada ne postoji ekspert sa domenskim znanjem, određivanje epsilon i minPts parametara je često dosta teško\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "** DODATNO **:\n",
    "\n",
    "* Izvršiti klasterizaciju u cilju eksplorative analize podataka nad bankovnim podacima ```data/bank.csv```. (4 boda)\n",
    "* Izvršiti klasterizaciju nad data setovima koji se nalaze na linku https://people.sc.fsu.edu/~jburkardt/datasets/hartigan/hartigan.html (3 boda).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
